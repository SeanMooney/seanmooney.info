---
title: "2025.2 Ptg"
date: 2025-04-15T09:26:03+01:00
tags: ["openstack", "ptg"]
draft: true
---

# Watcher

[PTG Etherpad](https://etherpad.opendev.org/p/r.9c40a9e71e93a4be96ebd3e0ad2d7bc4)

## Overview

The 2025.2 PTG was the Second PTG i have attended for watcher.
The last PTG had one session to cover all the technical debt required
to revive the project. This time we extended the ptg to 2 days.

## Day 1
### Tech Debt

#### [Croniter](https://etherpad.opendev.org/p/r.9c40a9e71e93a4be96ebd3e0ad2d7bc4#L39)

The PTG started with a disscussion about technical debt that emerged during the
epxoy cycle. This focused mainly on croniter, which is used by watcher to schedule
continuous audits based on cron strings. The Croniter libaray maintainer has decided
to stop maintaining this library and it is transitioning to a new maintainer.
While that gives watcher some breathing space, our usage is minimal, so we agreed
to drop the dependency by leveraging the existing functionality of apscheduler.

#### [Support status of datasources](https://etherpad.opendev.org/p/r.9c40a9e71e93a4be96ebd3e0ad2d7bc4#L51)
The second topic of the ptg focused on the support status of datasources and features of watcher in general.

As part of this discussion we touched on the idea of multiple levels of support:
- Core features (supported, tested in ci and ready for production use)
- Experimental features (supported but not tested in CI or may not be suitable for production use)
- Deprecated features (features that are no longer maintained and not suitable for production use)
- Unsupported features (features that are not supported by the project, but may work)

During this session we agreed to deprecate the support for the following features:
- Monasca
- Grafana
- Noisy Neighbour Strategy (l3 cache) 

Monasca support was dropped because it is no longer maintained and is officially being retired.

Grafana support was dropped because it is no longer maintained and has no existing ci coverage
under our new support levels it could be considered experimental as it may work under certain conditions
however since there is no ci and no active development it was agreed that we should drop support for this feature. 

The Current Noisy Neighbour Strategy (l3 cache) was deprected because it depends on metrics that are nolnger
avialable for several years. a dedicated session on how to replace it was held later in the ptg to discuss
how to replace it going forward.

Finally we wrapped up this topic by agreeing that we should create a support and testing matrix for watcher takeing inspiration from
https://docs.openstack.org/cinder/latest/reference/support-matrix.html or https://docs.openstack.org/nova/latest/user/support-matrix.html
The intent is to provide a way for users to see which features are supported, tested and suitable for production use cases.


#### [Eventlet Removal](https://etherpad.opendev.org/p/r.9c40a9e71e93a4be96ebd3e0ad2d7bc4#L73)

The final technical debt topic was Eventlet removal.
Watcher like many other OpenStack projects have been using eventlet since its inception, and it has become clear that eventlet
is no longer sustainable. While no concrete proposal was made for this topic, to start evaluating the
removal this cycle with some initial POCs and aim to remove eventlet in 2026.1


### [Workflow/API Improvements](https://etherpad.opendev.org/p/r.9c40a9e71e93a4be96ebd3e0ad2d7bc4#L88)

#### SKIPPED status for actions

This discusstion focused on the introduction of a new Skipped status for actions.
Today actions can be in one of the following states:
- pending
- ongoing
- succeeded
- failed

When constructing and executing an action plan it would be useful to extend the action states
to introduced a `skipped` state, which would allow users to mark an action as skipped, preventing it from being executed by the engine. The idea is that this can be used to skip actions that are known to fail or be undesirable to execute. Additionally if an actions preconditions cannot be met, the action can be marked as skipped instead of failed.

This lead to a larger discussion about the meaning of SUCCEEDED and FAILED for action plans in general.
https://etherpad.opendev.org/p/r.9c40a9e71e93a4be96ebd3e0ad2d7bc4#L108
We discussed that the current behavior where an action plan is considered succeeded when all actions are attempted
regardless of the success of the action is unintuitive and not in line with the state machine docs.

it was agree that if any action fails the overall action plan should be reported as failed.
Additionally in this context we agreed that if the SKIPPED state is added it should be considered as Succeeded, not Failed.

### [watcher-dashboard](https://etherpad.opendev.org/p/r.9c40a9e71e93a4be96ebd3e0ad2d7bc4#L135)

This sessions was relatively short and focused on 2 areas testing and missing functionality

#### Testing
We opened the session discussing the fact that the horizon plugin as almost no testing.
the net effect of this is every change need to be manually verified by the reviewer.
We have two options with regard to testing we can build on django's unittests integration
to validate the core logic, or we could build out selenium testing.
no general conclusion was arrived at but we resolved to follow up with the horizon
core team in the future for guidance. 

#### Exposing parameter in audits
This was simple and non controversial, today when creating a audit its not possible to
set the parameters filed via the horizon ui. we agreed this should be addresses as a speechless blueprint.

#### SKIPPED status
finally we discussed that if we extend the action status to model the new SKIPPED state we will need
to enhance the action plan dashboard to allow skipping an action and to display the skipped state when
a precondition fails.

### [Watcher cluster model collector improvement ideas](https://etherpad.opendev.org/p/r.9c40a9e71e93a4be96ebd3e0ad2d7bc4#L154)

This was arguably another tech debt discussion in the guise of an operator pain point.
For performance reasons watcher is build with a cached data model that periodically refreshes, combined
with integration with notification for near real-time updates.

During the session we reviewed https://bugs.launchpad.net/watcher/+bug/2104220 and resolved
to investigate if we are properly consuming the notifications form nova.
While we are consuming the notifications bug #2104220 asserts that we are perhaps using the
wrong fields to update the instance host on live migration resulting in the source host being updated
instead of the destination host.

other mitigation were discussed such as adding the ability to force refresh the model when executing an
audit, creating a new audit type to just run the refresh, or adjusting the collector interval.

### Day 1 Summary

Day one was pretty packed but we resolved to finish early when we reached the end of the agenda.

We resolved to proceed with 2 new specs for the SKIPPED sate and model collector plugins
and to treat the parameter enhancement to watcher-dashboard as a specless blueprint.
croniter will be replaced as a bug fix and we need to review the impact of bug: #2104220


## Day 2

### [Watcher and Nova's visible constraints](https://etherpad.opendev.org/p/r.9c40a9e71e93a4be96ebd3e0ad2d7bc4#L187)

Day 2 kicked off where day one ended on the topic of how watcher models nova instances
and the visibility of scheduler constraints. In the epoxy cycle we added a number of attributes
to the server show response that may be of interest to watcher, namely the scheduler hint and image properties.
additionally i noted that in a prior release we added the pinned AZ as a separate filed.
while extending the data model to provide this info would not be an api change we discussed that a spec
could be nice to have to document the changes but a specless blueprint could also be valid.


### [Noisy neighbour strategy](https://etherpad.opendev.org/p/r.9c40a9e71e93a4be96ebd3e0ad2d7bc4#L206)

This discussion happend as a sperate session on the second day but we touched on it on day 1
as part of the tech debt session. The TL;dr is since the exisitng noisy neighbour policy is non functional
and since we want to aovid upgrade impacts a new SLA goal with 1 or multiple strateies shoudl be created.
A spec to define the semantics should be created but at a high level the open questions are
* should we have one stagey per metric or a single parmaterised stagey
* which metrics shoudl we use as the key performance indicators to comptue if the sla is treshold is exceeded
  - cpu_steal
  - io_wait
  - cache_misses
  - others?

finally we agreed to deprecate the existing strategy this cycle for removal in 2026.2.
  

###  [Host Maintenance strategy new use case] https://etherpad.opendev.org/p/r.9c40a9e71e93a4be96ebd3e0ad2d7bc4#L223

The last feature session discussed the limitation with the current host maintenance strategy
the main limitations resolved around how to express if a instance should be stopped, cold migrated or live migrated.
we agreed to continue with the spec https://review.opendev.org/c/openstack/watcher-specs/+/943873
and that it was ok to add new parameters to allow specifying which actions are allowed for the audit.

This topic was reprised in the nova room on day 4 
TL;DR in the nova session we agreed to try to use server metadata to encode policies like live migratable.
this can be incorporated into the spec design as a follow up if there is time. 
i.e. instance can be annotated by adding lifecycle:livemigratable=True|False and similar lifeccyle: metadata
keys to express which actions may be taken by an external orchestrator like watcher.

### PTG wrappup

#### [contributor docs] (https://etherpad.opendev.org/p/r.9c40a9e71e93a4be96ebd3e0ad2d7bc4#L242)

The second to last session was a quick discussion of some of the document that watcher is missing
namely the `scope of the project` doc and the `chronological release guide`.
these are both important document for use to create going forward, as is ensuring our existing
contributor docs are up to date regarding when to file a bug vs blueprint vs spec.

#### [retro and adhoc planning](https://etherpad.opendev.org/p/r.9c40a9e71e93a4be96ebd3e0ad2d7bc4#L259)

To wrap up the ptg we had a short retro on how the last cycle went and any last minute topics.
traditionally one start the PTG with the retro rather then finishes but its better late then never.
we realised that one topic had not been raised which is the future of the core team
we briefly discussed that going forward we will review the core team member ship at the start of each slurp
release (i.e 2026.1) and remove inactive cores that have not participated in review, contribution or ptgs
since the prior slurp. As such no removal form the core team will be made for 2025.2 although new
addition may be made based on review activity over the cycle.

With that we we wrapped up the ptg with a reminder of our two cross project sessions with telemetry/horizon on day 3
and nova on day 4

# Telemetry/Horizon

TODO

# Nova

TODO
